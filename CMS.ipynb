{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpr4XreLQcMQ"
   },
   "outputs": [],
   "source": [
    "!echo -e \"beautifulsoup4==4.8.2\\ncertifi==2020.4.5.1\\nfeedparser==5.2.1\\niso8601==0.1.12\\nscrapy==2.0.0\\nstanza==1.0.1\\ntldextract==2.2.2\\ntorch==1.3.1\\n\" > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aw1MR727RTej"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sjdj78e3TjqT"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7udQTeAQOJP"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import stanza\n",
    "from bs4 import BeautifulSoup\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# This class gives the match rule for traversal for template given parsed beautiful soup object and author_name\n",
    "# and given a traversal rule identify the author\n",
    "class TraversalRule:\n",
    "    NER_TAG_PERSON = \"PERSON\"\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "    def __init__(self, parsed_page, author_name, traversal_rule):\n",
    "        self.parsed_page = parsed_page\n",
    "        self.author_name = author_name\n",
    "        self.traversal_rule = list()\n",
    "        if traversal_rule:\n",
    "            self.traversal_rule = traversal_rule\n",
    "\n",
    "    # https://stackoverflow.com/questions/54265391/find-all-end-nodes-that-contain-text-using-beautifulsoup4\n",
    "    def mark_if_leaf_with_text(self, node):\n",
    "        if node.name in [\"style\", \"script\", \"link\", \"meta\"]:\n",
    "            return False\n",
    "        if not node.text:\n",
    "            return False\n",
    "        elif len(node.find_all(text=False)) > 0:  # no other tags inside other than text\n",
    "            return False\n",
    "        node.leaf = True\n",
    "        return False\n",
    "\n",
    "    def is_leaf_nodes_with_people(self, node):\n",
    "        if not node.leaf:\n",
    "            return False\n",
    "        text = node.text\n",
    "        processed_text = self.nlp(text)\n",
    "        no_of_entities = len(processed_text.entities)\n",
    "        people = [ent for ent in processed_text.entities if ent.type == self.NER_TAG_PERSON]\n",
    "        no_of_people = len(people)\n",
    "        self.candidates = list()\n",
    "        if no_of_people > 0:\n",
    "            print(no_of_people, processed_text.entities)\n",
    "            node.no_of_people = no_of_people\n",
    "            node.no_of_entities = no_of_entities\n",
    "            node.people = people\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find_leaf_nodes_with_people(self, node):\n",
    "        self.mark_if_leaf_with_text(node)\n",
    "        return self.is_leaf_nodes_with_people(node)\n",
    "\n",
    "    def find_candidates(self):\n",
    "        logging.info(\"Finding Candidate Authors\")\n",
    "        # Use stanza to identify people in text, the nodes containing them and traversal for the leaf nodes\n",
    "        candidate_author_nodes_details = self.parsed_page.find_all(self.find_leaf_nodes_with_people)\n",
    "        candidate_authors = list()\n",
    "        for node in candidate_author_nodes_details:\n",
    "            candidate_author = dict()\n",
    "            # This condition may not hold\n",
    "            if node.no_of_people == 1:\n",
    "                candidate_author['author_entity'] = node.people[0].text\n",
    "                candidate_author['ancestors'] = [parent.name for parent in node.parents]\n",
    "                candidate_author['node_name'] = node.name\n",
    "                candidate_authors.append(candidate_author)\n",
    "        self.candidates = candidate_authors\n",
    "\n",
    "    def pick_traversal_from_author(self):\n",
    "        logging.info(\"Picking the traversal rule given author\")\n",
    "        self.find_candidates()\n",
    "        for candidate in self.candidates:\n",
    "            candidate_author_name = candidate['author_entity']\n",
    "            normal_name = self.get_normal_name()\n",
    "            logging.info(\"Candidate Author Name: {}, normal name {}\".format(candidate_author_name, normal_name))\n",
    "            if candidate_author_name == self.author_name or candidate_author_name == normal_name:\n",
    "                self.traversal_rule = candidate['ancestors']\n",
    "\n",
    "    def get_author_from_traversal(self):\n",
    "        logging.info(\"Picking the author from candidates based on the traversal rule\")\n",
    "\n",
    "        self.find_candidates()\n",
    "        candidates = self.candidates\n",
    "        if candidates:\n",
    "            for candidate in candidates:\n",
    "                if candidate['ancestors'] == self.traversal_rule:\n",
    "                    return candidate['author_entity']\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def get_normal_name(self):\n",
    "        if \",\" in self.author_name:\n",
    "            split = self.author_name.split(\",\")\n",
    "            not_last_name = split[1]\n",
    "            last_name = split[0]\n",
    "            normal_name = not_last_name + \" \" + last_name\n",
    "            return normal_name.strip()\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cec7e4bvT005"
   },
   "outputs": [],
   "source": [
    "with open(\"./resources/linkedin-origin-ab-testing-nicolai-kramer-jakobsen.html\", \"r\", encoding=\"UTF-8\") as fp:\n",
    "    html_content = fp.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "soup = BeautifulSoup(soup.prettify('utf-8'), 'lxml')  # some inputs are so messy that they affect the output\n",
    "t_rule = TraversalRule(soup, \"Nicolai Kramer Jakobsen\", None)\n",
    "t_rule.pick_traversal_from_author()\n",
    "print(t_rule.traversal_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWFx3C-ZP7xl"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import tldextract\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "ssl_context = ssl.SSLContext()\n",
    "ssl_context.load_verify_locations(certifi.where())\n",
    "http = urllib3.PoolManager(ssl_context=ssl_context)\n",
    "\n",
    "class AuthorTraversalRules:\n",
    "    persistence_type = \"json\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.author_traversal_rules = dict()\n",
    "        self.load_author_traversal_rules()\n",
    "\n",
    "    def load_author_traversal_rules(self):\n",
    "        with open(self.filename, \"r\") as fp:\n",
    "            self.author_traversal_rules =  json.load(fp)\n",
    "\n",
    "    def get_author_traversal_for_url(self, url):\n",
    "        extract_result = tldextract.extract(url)\n",
    "        host_url = extract_result.registered_domain\n",
    "        if host_url in self.author_traversal_rules:\n",
    "            return self.author_traversal_rules[host_url]\n",
    "        return None\n",
    "\n",
    "class FindAuthorWithTraversal:\n",
    "\n",
    "    def __init__(self, url, author_traversal_rule_for_site):\n",
    "        self.url = url\n",
    "        self.author_traversal_rule = author_traversal_rule_for_site\n",
    "        self.page_content = None\n",
    "\n",
    "    def load_page_content(self):\n",
    "        self.page_content = http.request('GET', self.url).data\n",
    "\n",
    "    def get_author(self):\n",
    "        self.load_page_content()\n",
    "        soup = BeautifulSoup(self.page_content, 'lxml')\n",
    "        soup = BeautifulSoup(soup.prettify('utf-8'), 'lxml')\n",
    "        t = TraversalRule(soup, None, self.author_traversal_rule)\n",
    "        return t.get_author_from_traversal()\n",
    "\n",
    "\n",
    "class FindAuthor:\n",
    "    domain_traversal_file = \"./resources/domain_traversal_rules-500.json\"\n",
    "    domain_traversal = AuthorTraversalRules(domain_traversal_file)\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        extracted = tldextract.extract(url)\n",
    "        site = extracted.registered_domain\n",
    "        print(site)\n",
    "        self.find_author = FindAuthorWithTraversal(self.url, self.domain_traversal.author_traversal_rules[site])\n",
    "\n",
    "    def get_author(self):\n",
    "        return self.find_author.get_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F70GJBL2P9eu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\meena\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.linkedin.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TraversalRule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c3a6a2f043c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFindAuthor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.linkedin.com/pulse/automating-user-creation-aws-sftp-service-transfer-arjun-dandagi/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_author\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-219f5ae80b0e>\u001b[0m in \u001b[0;36mget_author\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_author\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_author\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_author\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-219f5ae80b0e>\u001b[0m in \u001b[0;36mget_author\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTraversalRule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthor_traversal_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_author_from_traversal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TraversalRule' is not defined"
     ]
    }
   ],
   "source": [
    "print(FindAuthor(\"https://www.linkedin.com/pulse/automating-user-creation-aws-sftp-service-transfer-arjun-dandagi/\").get_author())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Qlqy30qUVwu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CMS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
